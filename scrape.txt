import requests
from bs4 import BeautifulSoup
import logging
from googlesearch import search

# Setting up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Function to perform a search on multiple search engines
def search_website(query):
    search_engines = ['google', 'bing', 'duckduckgo']
    results = []

    for engine in search_engines:
        try:
            if engine == 'google':
                results = list(search(query))
            elif engine == 'bing':
                results = list(search(query, search_engine="bing"))
            elif engine == 'duckduckgo':
                results = list(search(query, search_engine="duckduckgo"))
        except Exception as e:
            logging.error(f"Error searching with {engine}: {e}")
            continue  # Proceed with the next search engine if one fails

        if results:
            break  # If we get results, stop searching on other engines

    return results


# Function to filter the URLs based on relevancy (example: prioritize domain matching query)
def filter_urls(urls, query):
    filtered_urls = []
    for url in urls:
        if query.lower() in url.lower():
            filtered_urls.append(url)
    return filtered_urls


# Function to scrape content from the webpage
def scrape_website(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        return soup
    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching URL {url}: {e}")
        return None


# Function to extract title and description
def extract_title_and_description(soup):
    title = soup.title.string if soup.title else "No title found"
    description = None
    meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta',
                                                                              attrs={'property': 'og:description'})
    if meta_desc and meta_desc.get('content'):
        description = meta_desc.get('content')
    return title, description


# Function to extract a content preview and ignore irrelevant parts
def extract_content_preview(soup, char_limit=300):
    # Find the main content area, avoiding navigation, footer, or sidebar elements
    body_content = soup.find('body')

    if not body_content:
        return "No main content found."

    # Remove unwanted elements (like nav, footer)
    for unwanted_tag in ['header', 'footer', 'nav', 'aside']:
        for tag in body_content.find_all(unwanted_tag):
            tag.decompose()  # Remove these elements from the content

    # Get the visible text content
    text = body_content.get_text(separator=" ", strip=True)
    preview = text[:char_limit] + ('...' if len(text) > char_limit else '')
    return preview


# Function to extract subpage preview with content filtering
def extract_subpage_preview(url):
    soup = scrape_website(url)
    if soup:
        content_preview = extract_content_preview(soup)
        # Check if the content includes JavaScript-related messages and remove them
        if 'This site requires Javascript' in content_preview:
            content_preview = content_preview.replace('This site requires Javascript', '').strip()
        return content_preview
    return "No preview available."


# Main function to resolve website
def resolve_website(query):
    logging.info(f"Resolving website for query: {query}")
    search_results = search_website(query)

    if not search_results:
        logging.warning(f"No search results found for query: {query}")
        return None, None, None, None, None

    # Filter URLs
    relevant_urls = filter_urls(search_results, query)

    if not relevant_urls:
        logging.warning(f"No relevant URLs found for query: {query}")
        return None, None, None, None, None

    # Scrape the most relevant website
    main_url = relevant_urls[0]
    logging.info(f"Scraping main URL: {main_url}")
    main_page = scrape_website(main_url)

    if not main_page:
        logging.error(f"Failed to scrape main URL: {main_url}")
        return None, None, None, None, None

    # Extract title, description, and content preview
    title, description = extract_title_and_description(main_page)
    content_preview = extract_content_preview(main_page)

    # Crawl subpages for additional content
    subpages = crawl_subpages(main_url, depth=3)  # Set depth as needed
    subpage_previews = []
    for subpage in subpages[:5]:  # Limiting to 5 subpages for preview
        subpage_preview = extract_subpage_preview(subpage)
        subpage_previews.append((subpage, subpage_preview))

    return main_url, title, description, content_preview, subpage_previews


# Function to increase subpage crawling
def crawl_subpages(url, depth=2):
    if depth <= 0:
        return []

    subpages = []
    try:
        soup = scrape_website(url)
        if soup:
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if href.startswith('http'):
                    subpages.append(href)
            # Recursively crawl subpages
            for subpage in subpages[:5]:  # Limiting to 5 subpages to avoid overload
                subpages.extend(crawl_subpages(subpage, depth - 1))
    except Exception as e:
        logging.error(f"Error crawling {url}: {e}")

    return subpages


# Prompt for user input
query = input("Enter the website or topic you want to search: ")

# Example usage
main_url, title, description, content_preview, subpage_previews = resolve_website(query)

if main_url:
    print(f"Main page URL: {main_url}")
    print(f"Title: {title}")
    print(f"Description: {description}")
    print(f"Content Preview: {content_preview}")

    print("\nSubpages Found:")
    for subpage, preview in subpage_previews:
        print(f"{subpage}: {preview}")
else:
    print("Failed to resolve website.")


import os
import requests
from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# Load pretrained summarization model (Flan-T5)
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)

app = Flask(__name__)


# Summarize content using Flan-T5
def summarize_text(text, max_length=100):
    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)
    return summary[0]['summary_text']


# Evaluate a project proposal
def evaluate_proposal(proposal):
    scoring_criteria = {
        "Innovation": 0.4,
        "Feasibility": 0.3,
        "Impact": 0.2,
        "Alignment with Trends": 0.1
    }
    score = 0
    for criterion, weight in scoring_criteria.items():
        if criterion.lower() in proposal.lower():
            score += 10 * weight  # Simulated score
    return round(score, 2)


# Scrape for academic papers
def find_academic_papers(topic):
    search_url = f"https://scholar.google.com/scholar?q={topic.replace(' ', '+')}"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    results = []

    for entry in soup.select('.gs_ri'):
        title = entry.select_one('.gs_rt a').text if entry.select_one('.gs_rt a') else "No title found"
        link = entry.select_one('.gs_rt a')['href'] if entry.select_one('.gs_rt a') else "No link"
        snippet = entry.select_one('.gs_rs').text if entry.select_one('.gs_rs') else "No snippet"
        results.append({"title": title, "link": link, "snippet": snippet})

    return results


# Scrape for patents
def find_patents(keyword):
    search_url = f"https://patents.google.com/?q={keyword.replace(' ', '+')}"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    results = []

    for entry in soup.find_all('search-result-item', limit=5):
        title = entry.find('span', class_='title').text if entry.find('span', class_='title') else "No title"
        link = "https://patents.google.com" + entry.a['href'] if entry.a else "No link"
        results.append({"title": title, "link": link})

    return results


# API Endpoints
@app.route('/summarize', methods=['POST'])
def summarize():
    data = request.json
    text = data.get("text", "")
    if not text:
        return jsonify({"error": "No text provided"}), 400
    summary = summarize_text(text)
    return jsonify({"summary": summary})


@app.route('/evaluate', methods=['POST'])
def evaluate():
    data = request.json
    proposal = data.get("proposal", "")
    if not proposal:
        return jsonify({"error": "No proposal provided"}), 400
    score = evaluate_proposal(proposal)
    return jsonify({"score": score})


@app.route('/papers', methods=['GET'])
def papers():
    topic = request.args.get("topic", "")
    if not topic:
        return jsonify({"error": "No topic provided"}), 400
    papers = find_academic_papers(topic)
    return jsonify({"papers": papers})


@app.route('/patents', methods=['GET'])
def patents():
    keyword = request.args.get("keyword", "")
    if not keyword:
        return jsonify({"error": "No keyword provided"}), 400
    patents = find_patents(keyword)
    return jsonify({"patents": patents})


# Run the app
if __name__ == '__main__':
    app.run(debug=True, port=5000)
